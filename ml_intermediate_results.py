# -*- coding: utf-8 -*-
"""ML_intermediate_results

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_ogy9G19MfyQ7wpcaRn5rFRUDo4qEYWi
"""

from google.colab import drive
drive.mount('/content/drive/')

# Written by Bibaswan Khadka, Rodrigo Cavero Blades, Pushpendra Singh. Code for our best approach. 

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report,confusion_matrix, accuracy_score
from sklearn.svm import LinearSVC
from sklearn.ensemble import VotingClassifier
from sklearn.calibration import CalibratedClassifierCV
import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
import re
nltk.download('stopwords')

# Function to import json data. Path must be a string.
def importdata(path):
  return pd.read_json(path, lines=True)

# Input - Pandas dataframe
# Output - Returns a pandas dataframe with the fields we are not using removed, and remove rows that have NULL values
def clean_dataframe(dataframe):
  df = dataframe
  cols = ["reviewerID", "reviewerName","verified","unixReviewTime","reviewTime","vote","image","style"]
  df.drop(cols, axis=1, inplace=True)
  df = df[df['reviewText'].notna()]
  df = df[df['summary'].notna()]
  return df

# Function to create a custom stopwords dictionary. Necessary as the NLTK dictionary contains some words we do not want blocked. 
def create_customStopWords():
  default_stop_words = set(stopwords.words('english'))
  removefromlist = { 'not'}
  custom_stop_words = set([word for word in default_stop_words if word not in removefromlist])
  return custom_stop_words

# Function to steam, remove stop words, punctuation, and concatenate summary and review text.
def featureize(reviewText, summary):
  stemmer = nltk.stem.SnowballStemmer('english')
  custom_stop_words = create_customStopWords()
  combined_string = (str(summary)+ " " + str(reviewText)).lower()
  cleaned = re.sub(r'[?|!|\'|"|#]',r'',combined_string)
  cleaned = re.sub(r'[.|,|)|(|\|/]',r' ',cleaned) # Clean punctutation and special characters
  words = [stemmer.stem(word) for word in cleaned.split() if word not in custom_stop_words]
  words = ' '.join(words)
  return words

# Creates a new field called combinedcleantext with the new concatenated and manipulated strings
def create_features(dataframe):
  df = dataframe
  df['combinedcleantext'] = df.apply(lambda row: featureize(row['reviewText'],row['summary']),axis=1)
  df = df[df['combinedcleantext'].notna()]
  return df

# Function to print out metrics to assess classifier accuracy
def get_report(y_test, predictions):
  print(classification_report(y_test, predictions))
  print(confusion_matrix(y_test, predictions))

# Function to optimize the alpha hyperparameter in Multinomial Naive Bayes. 
# Most efficient to run once to get the optimal value, then hardcode it in, so that it does not have to be run again. 
def optimizeNaiveBayesAlpha(X_train, y_train):
  potential_values = np.arange(1,100,0.5)
  cv_scores = []
  for value in potential_values:
    mnb = MultinomialNB(alpha=value)
    scores = cross_val_score(mnb, X_train, y_train, cv = 10, scoring = 'accuracy')
    cv_scores.append(scores.mean())
  MSE = [1 - x for x in cv_scores]
  optimal_alpha = potential_values[MSE.index(min(MSE))]
  print('Optimal number = %d.' % optimal_alpha)
  return optimal_alpha

# Input- X and y values. X value needs to have tfidvectorizer called on it. 
# Output- The trained predictive model
# returns the hardvoting version of the voting classifier
def createHardVoting_classifier(X_train, y_train):
  NB = MultinomialNB(alpha=1)
  SVM = LinearSVC(class_weight='balanced')
  SVMwrapper = CalibratedClassifierCV(SVM) 
  ensemble = VotingClassifier(estimators = [('MultinomialNB', NB), ('SVM',SVMwrapper)], voting='hard')
  ensemble.fit(X_train, y_train)
  return ensemble

# Input- X and y values. X value needs to have tfidvectorizer called on it. 
# Output- The trained predictive model
def createAndTrain_classifier(X_train, y_train):
  NB = MultinomialNB(alpha=1)
  SVM = LinearSVC(class_weight='balanced')
  SVMwrapper = CalibratedClassifierCV(SVM) # LinearSVC normally does not have a function to return probability estimates, so it must be wrapped
  ensemble = VotingClassifier(estimators = [('MultinomialNB', NB), ('SVM',SVMwrapper)], voting='soft')
  ensemble.fit(X, y)
  return ensemble

# Input- X which has had tfidvectorizer called on it, a dataframe, and the classifier
# Output - results dataframe with 1 or 0 for each product. 1 = is awesome, 0 = not awesome
def AwesomeOrNotPredictions(X_test, dataframe, classifier):
  df_test = dataframe
  predictions = classifier.predict(X_test)
  df_test['avg_score_prediction'] = predictions
  results = df_test[['asin', 'avg_score_prediction']].groupby('asin').mean()
  results['prediction'] = np.where(results['avg_score_prediction']>4.7, 1, 0)
  results.drop('avg_score_prediction', axis=1, inplace=True)
  results.sort_values('asin')
  return results

# Train the model
df = importdata('/content/drive/My Drive/Automotive_Reviews_training.json')
df = clean_dataframe(df)
df = create_features(df)

tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,3))
X = tfidf.fit_transform(df['combinedcleantext'])
y = df['overall']
classifier = createAndTrain_classifier(X,y)

# Import test data and predict
df_test = pd.read_json('/content/drive/My Drive/Automotive_Reviews_test.json', lines=True)
df_test = clean_dataframe(df_test)

df_test = create_features(df_test)
df_test.head(5)

X_test = tfidf.transform(df_test['combinedcleantext'])
results = AwesomeOrNotPredictions(X_test,df_test, classifier)
results.head(10)

# Export results
results.to_csv('/content/drive/My Drive/automotive_intermediate_result.csv')

# Basic test code
# df = importdata('/content/drive/My Drive/Automotive_Reviews_training.json')
# df = clean_dataframe(df)

# df = create_features(df)

# tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,3))
# X = tfidf.fit_transform(df['combinedcleantext'])
# y= df['overall']
# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)

# test_classifier = createTest_classifier(X_train, y_train)

# predictions = test_classifier.predict(X_test)
# get_report(y_test,predictions)